{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cbdbf6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages\\pandas\\__init__.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[1;32mc:\\Users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages\\pandas\\compat\\__init__.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     19\u001b[0m     is_numpy_dev,\n\u001b[0;32m     20\u001b[0m     np_version_under1p21,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     pa_version_under1p01,\n\u001b[0;32m     24\u001b[0m     pa_version_under2p0,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     pa_version_under9p0,\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32mc:\\Users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages\\pandas\\compat\\numpy\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" support numpy compatibility across versions \"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# numpy versioning\u001b[39;00m\n\u001b[0;32m      7\u001b[0m _np_version \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39m__version__\n",
      "File \u001b[1;32mc:\\Users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages\\pandas\\util\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# pyright: reportUnusedImport = false\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     Appender,\n\u001b[0;32m      4\u001b[0m     Substitution,\n\u001b[0;32m      5\u001b[0m     cache_readonly,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhashing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     hash_array,\n\u001b[0;32m     10\u001b[0m     hash_pandas_object,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(name):\n",
      "File \u001b[1;32mc:\\Users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages\\pandas\\util\\_decorators.py:14\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     Any,\n\u001b[0;32m      8\u001b[0m     Callable,\n\u001b[0;32m      9\u001b[0m     Mapping,\n\u001b[0;32m     10\u001b[0m     cast,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproperties\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cache_readonly\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     F,\n\u001b[0;32m     17\u001b[0m     T,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n",
      "File \u001b[1;32mc:\\Users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages\\pandas\\_libs\\__init__.py:13\u001b[0m\n\u001b[0;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaTType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m ]\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     NaT,\n\u001b[0;32m     16\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     iNaT,\n\u001b[0;32m     22\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages\\pandas\\_libs\\interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 한글 폰트 설정 (Mac)\n",
    "# plt.rc('font', family='AppleGothic')\n",
    "# 한글 폰트 설정 (Windows)\n",
    "# plt.rc('font', family='Malgun Gothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False # 마이너스 기호 깨짐 방지\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "try:\n",
    "    train_df = pd.read_csv('../data/train.csv')\n",
    "    print(\"데이터 불러오기 완료.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"파일 경로를 다시 확인해주세요.\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"1. 데이터 기본 정보 및 결측치 확인\")\n",
    "print(\"=\"*50)\n",
    "train_df.info()\n",
    "print(\"\\n결측치 비율:\\n\", train_df.isnull().sum() / len(train_df) * 100)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"2. 결측치 시각화\")\n",
    "print(\"=\"*50)\n",
    "msno.matrix(train_df)\n",
    "plt.title('Missingness Matrix')\n",
    "plt.show()\n",
    "\n",
    "msno.bar(train_df)\n",
    "plt.title('Missingness Bar Chart')\n",
    "plt.show()\n",
    "\n",
    "msno.heatmap(train_df)\n",
    "plt.title('Missingness Heatmap (Correlation between missing values)')\n",
    "plt.show()\n",
    "\n",
    "# 3. 데이터 유형별 분리\n",
    "numerical_cols = train_df.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = train_df.select_dtypes(include='object').columns.tolist()\n",
    "numerical_cols.remove('stress_score')\n",
    "categorical_cols.remove('ID')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"3. 수치형 변수 EDA\")\n",
    "print(\"=\"*50)\n",
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 히스토그램과 KDE\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(train_df[col], kde=True)\n",
    "    plt.title(f'{col} Distribution')\n",
    "    \n",
    "    # 박스 플롯으로 이상치 확인\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=train_df[col])\n",
    "    plt.title(f'{col} Outliers')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 수치형 변수 간의 상관관계\n",
    "print(\"\\n수치형 변수 간의 상관관계\")\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(train_df[numerical_cols + ['stress_score']].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"4. 범주형 변수 EDA\")\n",
    "print(\"=\"*50)\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n** {col} 변수 분석 **\")\n",
    "    print(\"빈도:\\n\", train_df[col].value_counts(dropna=False))\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # 빈도 시각화\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.countplot(y=train_df[col], order=train_df[col].value_counts().index)\n",
    "    plt.title(f'{col} Distribution')\n",
    "\n",
    "    # 타겟 변수(stress_score)와의 관계\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x='stress_score', y=col, data=train_df)\n",
    "    plt.title(f'{col} vs Stress Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"5. 결측치가 있는 변수와 타겟 변수의 관계\")\n",
    "print(\"=\"*50)\n",
    "missing_cols = ['medical_history', 'family_medical_history', 'edu_level', 'mean_working']\n",
    "for col in missing_cols:\n",
    "    temp_df = train_df.copy()\n",
    "    \n",
    "    # 이 부분을 수정했어! 불리언 값을 정수(0, 1)로 변환\n",
    "    temp_df[f'{col}_is_missing'] = temp_df[col].isnull().astype(int)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(x='stress_score', y=f'{col}_is_missing', data=temp_df)\n",
    "    plt.title(f'Stress Score vs {col} Missingness')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"6. LightGBM 모델을 활용한 변수 중요도 (Feature Importance)\")\n",
    "print(\"=\"*50)\n",
    "# 결측치 처리 (간단하게)\n",
    "temp_df = train_df.copy()\n",
    "for col in ['medical_history', 'family_medical_history', 'edu_level']:\n",
    "    temp_df[col] = temp_df[col].fillna('unknown')\n",
    "temp_df['mean_working'] = temp_df['mean_working'].fillna(temp_df['mean_working'].median())\n",
    "temp_df = temp_df.drop('ID', axis=1)\n",
    "\n",
    "# 범주형 변수 레이블 인코딩\n",
    "for col in temp_df.select_dtypes(include='object').columns:\n",
    "    le = LabelEncoder()\n",
    "    temp_df[col] = le.fit_transform(temp_df[col])\n",
    "\n",
    "X = temp_df.drop('stress_score', axis=1)\n",
    "y = temp_df['stress_score']\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42, n_estimators=100)\n",
    "lgb_model.fit(X, y)\n",
    "\n",
    "feature_importance = pd.DataFrame({'feature': X.columns, 'importance': lgb_model.feature_importances_})\n",
    "feature_importance = feature_importance.sort_values(by='importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "plt.title('Feature Importance from LightGBM')\n",
    "plt.show()\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce36a628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.2.8-cp310-cp310-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting graphviz (from catboost)\n",
      "  Downloading graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages (from catboost) (3.7.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.16.0 in c:\\users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages (from catboost) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages (from catboost) (1.5.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages (from catboost) (1.11.3)\n",
      "Collecting plotly (from catboost)\n",
      "  Downloading plotly-6.2.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: six in c:\\users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages (from matplotlib->catboost) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages (from matplotlib->catboost) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages (from matplotlib->catboost) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages (from matplotlib->catboost) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages (from matplotlib->catboost) (3.2.3)\n",
      "Collecting narwhals>=1.15.1 (from plotly->catboost)\n",
      "  Downloading narwhals-2.0.1-py3-none-any.whl.metadata (11 kB)\n",
      "Downloading catboost-1.2.8-cp310-cp310-win_amd64.whl (102.5 MB)\n",
      "   ---------------------------------------- 0.0/102.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 3.1/102.5 MB 16.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 9.2/102.5 MB 23.8 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 9.7/102.5 MB 19.5 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 11.5/102.5 MB 15.0 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 12.1/102.5 MB 12.2 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 17.6/102.5 MB 14.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 26.0/102.5 MB 18.5 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 35.1/102.5 MB 21.9 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 44.3/102.5 MB 24.5 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 46.1/102.5 MB 24.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 47.2/102.5 MB 22.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 55.1/102.5 MB 22.8 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 60.0/102.5 MB 22.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 68.7/102.5 MB 24.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 72.4/102.5 MB 24.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 76.3/102.5 MB 23.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 82.3/102.5 MB 24.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 86.5/102.5 MB 23.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 89.7/102.5 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 94.6/102.5 MB 23.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 98.8/102.5 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  102.2/102.5 MB 23.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 102.5/102.5 MB 23.1 MB/s eta 0:00:00\n",
      "Downloading graphviz-0.21-py3-none-any.whl (47 kB)\n",
      "Downloading plotly-6.2.0-py3-none-any.whl (9.6 MB)\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 6.0/9.6 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.6/9.6 MB 24.0 MB/s eta 0:00:00\n",
      "Downloading narwhals-2.0.1-py3-none-any.whl (385 kB)\n",
      "Installing collected packages: narwhals, graphviz, plotly, catboost\n",
      "\n",
      "   ---------------------------------------- 0/4 [narwhals]\n",
      "   ---------------------------------------- 0/4 [narwhals]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   -------------------- ------------------- 2/4 [plotly]\n",
      "   ------------------------------ --------- 3/4 [catboost]\n",
      "   ------------------------------ --------- 3/4 [catboost]\n",
      "   ------------------------------ --------- 3/4 [catboost]\n",
      "   ------------------------------ --------- 3/4 [catboost]\n",
      "   ------------------------------ --------- 3/4 [catboost]\n",
      "   ------------------------------ --------- 3/4 [catboost]\n",
      "   ---------------------------------------- 4/4 [catboost]\n",
      "\n",
      "Successfully installed catboost-1.2.8 graphviz-0.21 narwhals-2.0.1 plotly-6.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3f73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Using cached alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages (from optuna) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages (from optuna) (25.0)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
      "  Downloading sqlalchemy-2.0.42-cp310-cp310-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages (from optuna) (4.66.1)\n",
      "Collecting PyYAML (from optuna)\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Using cached mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
      "Collecting tomli (from alembic>=1.5.0->optuna)\n",
      "  Using cached tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting greenlet>=1 (from sqlalchemy>=1.4.2->optuna)\n",
      "  Downloading greenlet-3.2.3-cp310-cp310-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\rladu\\anaconda3\\envs\\team_project_py310\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Collecting MarkupSafe>=0.9.2 (from Mako->alembic>=1.5.0->optuna)\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
      "Using cached alembic-1.16.4-py3-none-any.whl (247 kB)\n",
      "Downloading sqlalchemy-2.0.42-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 19.8 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.2.3-cp310-cp310-win_amd64.whl (296 kB)\n",
      "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Using cached mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "Using cached tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: tomli, PyYAML, MarkupSafe, greenlet, colorlog, sqlalchemy, Mako, alembic, optuna\n",
      "\n",
      "   ---------------------- ----------------- 5/9 [sqlalchemy]\n",
      "   ---------------------- ----------------- 5/9 [sqlalchemy]\n",
      "   ---------------------- ----------------- 5/9 [sqlalchemy]\n",
      "   ---------------------- ----------------- 5/9 [sqlalchemy]\n",
      "   ---------------------- ----------------- 5/9 [sqlalchemy]\n",
      "   ---------------------- ----------------- 5/9 [sqlalchemy]\n",
      "   -------------------------- ------------- 6/9 [Mako]\n",
      "   ------------------------------- -------- 7/9 [alembic]\n",
      "   ----------------------------------- ---- 8/9 [optuna]\n",
      "   ----------------------------------- ---- 8/9 [optuna]\n",
      "   ---------------------------------------- 9/9 [optuna]\n",
      "\n",
      "Successfully installed Mako-1.3.10 MarkupSafe-3.0.2 PyYAML-6.0.2 alembic-1.16.4 colorlog-6.9.0 greenlet-3.2.3 optuna-4.4.0 sqlalchemy-2.0.42 tomli-2.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36f00901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "submission_improved.csv 파일이 성공적으로 생성되었습니다! 제출해서 점수를 확인해 보세요.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "try:\n",
    "    train_df = pd.read_csv('../data/train.csv')\n",
    "    test_df = pd.read_csv('../data/test.csv')\n",
    "    submission_df = pd.read_csv('../data/sample_submission.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"파일 경로를 다시 확인해주세요.\")\n",
    "    exit()\n",
    "\n",
    "# 2. 전처리 및 파생변수 생성 함수 (개선된 버전)\n",
    "def preprocess_and_feature_engineer_improved(df, scaler=None, fit_scaler=False):\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # 이상치 처리\n",
    "    df_processed.loc[df_processed['bone_density'] < 0, 'bone_density'] = 0\n",
    "\n",
    "    # 결측치 처리 (그룹별 중앙값 사용)\n",
    "    for col in ['medical_history', 'family_medical_history', 'edu_level']:\n",
    "        df_processed[col] = df_processed[col].fillna('unknown')\n",
    "    \n",
    "    df_processed['mean_working'] = df_processed.groupby(['smoke_status', 'edu_level'])['mean_working'].transform(lambda x: x.fillna(x.median()))\n",
    "    df_processed['mean_working'] = df_processed['mean_working'].fillna(df_processed['mean_working'].median())\n",
    "\n",
    "    # 파생 변수 생성\n",
    "    df_processed['BMI'] = df_processed['weight'] / (df_processed['height'] / 100) ** 2\n",
    "    df_processed['Hypertension_flag'] = ((df_processed['systolic_blood_pressure'] >= 140) | (df_processed['diastolic_blood_pressure'] >= 90)).astype(int)\n",
    "    df_processed['Pulse_Pressure'] = df_processed['systolic_blood_pressure'] - df_processed['diastolic_blood_pressure']\n",
    "    df_processed['bp_interaction'] = df_processed['systolic_blood_pressure'] * df_processed['diastolic_blood_pressure']\n",
    "    df_processed['cholesterol_glucose_ratio'] = df_processed['cholesterol'] / (df_processed['glucose'] + 1)\n",
    "    \n",
    "    bins = [0, 8, 12, 16, 24]\n",
    "    labels = ['~8h', '8~12h', '12~16h', '16h~']\n",
    "    df_processed['Working_Hour_Group'] = pd.cut(df_processed['mean_working'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # 범주형 변수 원-핫 인코딩\n",
    "    categorical_cols = ['gender', 'activity', 'smoke_status', 'sleep_pattern',\n",
    "                        'medical_history', 'family_medical_history', 'edu_level',\n",
    "                        'Working_Hour_Group']\n",
    "    df_processed = pd.get_dummies(df_processed, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # 변수 스케일링\n",
    "    numerical_cols = ['age', 'height', 'weight', 'systolic_blood_pressure', \n",
    "                      'diastolic_blood_pressure', 'cholesterol', 'glucose', \n",
    "                      'bone_density', 'mean_working', 'BMI', \n",
    "                      'Pulse_Pressure', 'bp_interaction', 'cholesterol_glucose_ratio']\n",
    "\n",
    "    numerical_cols_for_scaling = [col for col in numerical_cols if col in df_processed.columns]\n",
    "    \n",
    "    if fit_scaler:\n",
    "        scaler = StandardScaler()\n",
    "        df_processed[numerical_cols_for_scaling] = scaler.fit_transform(df_processed[numerical_cols_for_scaling])\n",
    "        return df_processed, scaler\n",
    "    else:\n",
    "        df_processed[numerical_cols_for_scaling] = scaler.transform(df_processed[numerical_cols_for_scaling])\n",
    "        return df_processed, scaler\n",
    "\n",
    "# 3. 데이터에 함수 적용\n",
    "train_final, scaler_fit = preprocess_and_feature_engineer_improved(train_df.drop('ID', axis=1), fit_scaler=True)\n",
    "test_final, _ = preprocess_and_feature_engineer_improved(test_df.drop('ID', axis=1), scaler=scaler_fit)\n",
    "\n",
    "# 4. 학습 데이터와 테스트 데이터의 컬럼 일치시키기 및 중요도 낮은 변수 제거\n",
    "train_cols = set(train_final.drop('stress_score', axis=1).columns)\n",
    "test_cols = set(test_final.columns)\n",
    "common_cols = list(train_cols.intersection(test_cols))\n",
    "\n",
    "# 중요도가 낮은 변수 리스트 (기존 코드와 동일)\n",
    "low_importance_features = ['sleep_pattern_sleep difficulty', 'activity_moderate', 'mean_working']\n",
    "final_cols = [col for col in common_cols if col not in low_importance_features]\n",
    "\n",
    "X_train = train_final[final_cols]\n",
    "y_train = train_final['stress_score']\n",
    "X_test = test_final[final_cols]\n",
    "\n",
    "# 5. 모델 학습 및 예측 (기존 코드와 동일)\n",
    "# LightGBM 모델\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=40\n",
    ")\n",
    "lgb_model.fit(X_train, y_train)\n",
    "lgb_predictions = lgb_model.predict(X_test)\n",
    "\n",
    "# XGBoost 모델\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=7,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "# 6. 두 모델의 예측값에 가중치를 부여해 최종 예측값 생성 (기존 코드와 동일)\n",
    "weight_lgb = 0.6\n",
    "weight_xgb = 0.4\n",
    "weighted_predictions = (weight_lgb * lgb_predictions) + (weight_xgb * xgb_predictions)\n",
    "\n",
    "# 7. 제출 파일 생성\n",
    "submission_df['stress_score'] = weighted_predictions\n",
    "submission_df.to_csv('../data/submission_improved.csv', index=False)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"submission_improved.csv 파일이 성공적으로 생성되었습니다! 제출해서 점수를 확인해 보세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9956477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "submission_weighted_final_corrected.csv 파일이 성공적으로 생성되었습니다! 이 파일을 제출해서 점수를 확인해 봐.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "try:\n",
    "    train_df = pd.read_csv('../data/train.csv')\n",
    "    test_df = pd.read_csv('../data/test.csv')\n",
    "    submission_df = pd.read_csv('../data/sample_submission.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"파일 경로를 다시 확인해주세요.\")\n",
    "    exit()\n",
    "\n",
    "# 2. 전처리 및 파생변수 생성 함수\n",
    "def preprocess_and_feature_engineer(df):\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # gender 열 제거\n",
    "    if 'gender' in df_processed.columns:\n",
    "        df_processed = df_processed.drop('gender', axis=1)\n",
    "\n",
    "    df_processed.loc[df_processed['bone_density'] < 0, 'bone_density'] = 0\n",
    "    df_processed[['medical_history', 'family_medical_history', 'edu_level']] = df_processed[['medical_history', 'family_medical_history', 'edu_level']].fillna('unknown')\n",
    "    df_processed['mean_working'] = df_processed.groupby(['smoke_status', 'edu_level'])['mean_working'].transform(lambda x: x.fillna(x.median()))\n",
    "    df_processed['mean_working'] = df_processed['mean_working'].fillna(df_processed['mean_working'].median())\n",
    "    \n",
    "    # gender를 제외한 범주형 변수 처리\n",
    "    categorical_cols = ['activity', 'smoke_status', 'sleep_pattern',\n",
    "                        'medical_history', 'family_medical_history', 'edu_level']\n",
    "    df_processed = pd.get_dummies(df_processed, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    df_processed['BMI'] = df_processed['weight'] / (df_processed['height'] / 100) ** 2\n",
    "    # Hypertension_flag 파생변수 생성 부분 제외\n",
    "    df_processed['Pulse_Pressure'] = df_processed['systolic_blood_pressure'] - df_processed['diastolic_blood_pressure']\n",
    "    df_processed['bp_interaction'] = df_processed['systolic_blood_pressure'] * df_processed['diastolic_blood_pressure']\n",
    "    df_processed['cholesterol_glucose_ratio'] = df_processed['cholesterol'] / (df_processed['glucose'] + 1)\n",
    "    return df_processed\n",
    "\n",
    "# 3. 데이터에 함수 적용\n",
    "train_final = preprocess_and_feature_engineer(train_df.copy())\n",
    "test_final = preprocess_and_feature_engineer(test_df.copy())\n",
    "\n",
    "# 4. 학습 데이터와 테스트 데이터의 컬럼 일치시키기\n",
    "train_cols = set(train_final.columns)\n",
    "test_cols = set(test_final.columns)\n",
    "common_cols = list(train_cols.intersection(test_cols))\n",
    "common_cols = [col for col in common_cols if col not in ['ID', 'stress_score']]\n",
    "\n",
    "# 중요도가 낮은 변수 리스트 (제거)\n",
    "low_importance_features = ['sleep_pattern_sleep difficulty', 'activity_moderate', 'mean_working']\n",
    "final_cols = [col for col in common_cols if col not in low_importance_features]\n",
    "\n",
    "X_train = train_final[final_cols]\n",
    "y_train = train_final['stress_score']\n",
    "X_test = test_final[final_cols]\n",
    "\n",
    "# 5. 모델 학습 및 예측\n",
    "# LightGBM 모델 (최적의 파라미터 적용)\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=40\n",
    ")\n",
    "lgb_model.fit(X_train, y_train)\n",
    "lgb_predictions = lgb_model.predict(X_test)\n",
    "\n",
    "# XGBoost 모델 (최적의 파라미터 적용)\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=7,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "# 6. 두 모델의 예측값에 가중치를 부여해 최종 예측값 생성\n",
    "# LightGBM과 XGBoost의 예측값을 가중 평균\n",
    "weight_lgb = 0.6\n",
    "weight_xgb = 0.4\n",
    "weighted_predictions = (weight_lgb * lgb_predictions) + (weight_xgb * xgb_predictions)\n",
    "\n",
    "# 7. 제출 파일 생성\n",
    "submission_df['stress_score'] = weighted_predictions\n",
    "submission_df.to_csv('../data/submission_weighted_final_corrected.csv', index=False)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"submission_weighted_final_corrected.csv 파일이 성공적으로 생성되었습니다! 이 파일을 제출해서 점수를 확인해 봐.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
